{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1wpqCzPIa9Xf-nZgTPTyvjZSsU2cqw333","authorship_tag":"ABX9TyOFogawxt1t6tqKAFdwCPuc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#4. Matrix Factorization(MF) 기반 추천\n","\n","||메모리 기반 알고리즘|모델 기반 알고리즘|\n","|---|---|---|\n","|설명|메모리에 있는 데이터를 계산해서 <br> 추천하는 방식|데이터로부터 미리 모델을 구성 후, <br> 필요 시 추천하는 방식|\n","|특징|개별 사용자 데이터 집중|전체 사용자 패턴 집중|\n","|장점|원래 데이터에 충실하게 사용|대규모 데이터에 빠르게 반응|\n","|단점|대규모 데이터에 느리게 반응|모델 생성 과정 오래 걸림|\n","|예시|CF 기반 추천 알고리즘|MF 기반 추천 알고리즘, 딥러닝|"],"metadata":{"id":"xwXDeRu_AXBe"}},{"cell_type":"markdown","source":["## 4.1. Matrix Factorization(MF) 방식의 원리\n","* Matrix Factorization : 행렬 요인화/분해\n","* 평가, 사용자, 아이템으로 구성된 하나의 행렬을, 두 개의 행렬로 분해\n","* R ≈ P × Q.T = R^ (예상 평점)\n","  * Rating matrix R - M×N 차원\n","  * User latent matrix 사용자와 사용자 잠재 요인 행렬 P - M×K 차원\n","  * Item latent matrix 아이템과 아이템 잠재 요인 행렬 Q - N×K 차원\n","* CF에서는 사용자와 아이템, 평점으로 이루어진 full-matrix R 이용"],"metadata":{"id":"LW3hwHy2CIOb"}},{"cell_type":"markdown","source":["## 4.2. SGD(Stochastic Gradient Decent)를 사용한 MF 알고리즘\n"," * SGD를 사용해 MF의 P와 Q 행렬을 구하는 게 최종 목표  \n","\n","### 4.2.1. MF 알고리즘 개념적 설명\n","> 1. 잠재 요인 개수 K 선택\n","> 2. P, Q 행렬 초기화  \n",">\n","> [ 반복 ]  \n","> > 3. 예측 평점 R_hat(= P×Q.T) 계산\n","> > 4. 실제 R과 R_hat 간 오차 계산 및 P, Q 수정 (오차 감소 위함)\n","> > → 가장 중요한 단계\n","> > 5. 기준 오차 도달 확인\n","\n","* MF의 핵심 : P와 Q 잘 분해하기\n","  * 주어진 사용자와 아이템의 관계를 잘 설명할 수 있도록  \n","\n","### 4.2.2. SGD : Stochastic Gradient Decent\n","* 예측 오차를 줄이기위한 P, Q 업데이트\n","  * 예측 오차 제곱의 편미분 값 사용\n","  * 학습률(learning rate) α 알파 활용\n","* Overfitting 과적합 방지\n","  * 정규화 고려\n","    * 정규화 항(Regulation term) 추가\n","    * 정교화 계수 β\n","  * 경향성 고려\n","    * 사용자와 아이템의 경향성 문제\n","      * 전체 평균 b\n","      * 전체 평균을 제거한 후 사용자 i의 평가 경향 bu[i]\n","        * 사용자 i 평균과 전체 평균의 차이\n","      * 전체 평균을 제거한 후 아이템 j의 평가 경향 bi[j]\n","        * 아이템 j의 평균과 전체 평균의 차이\n","    * CF에서는 사용자와 아이템 별로 평가 경향이 한 번에 계산되었는데  \n","    MF에서는 계산할 때마다 오차를 최소화하도록 bu[i]와 bi[j] 계속 업데이트\n","\n","\n"],"metadata":{"id":"IUuvAt6XLB7Y"}},{"cell_type":"markdown","source":["## 4.3. SGD를 사용한 MF 기본 알고리즘"],"metadata":{"id":"UToSmRj_cb18"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","base_src = \"drive/MyDrive/RecoSys/python-recosys/Data\"\n","u_data_src = os.path.join(base_src, \"u.data\")\n","r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n","ratings = pd.read_csv(u_data_src, sep = \"\\t\", names = r_cols, encoding = \"latin-1\")\n","ratings = ratings[[\"user_id\", \"movie_id\", \"rating\"]].astype(int)"],"metadata":{"id":"5BoxonY5cZyz","executionInfo":{"status":"ok","timestamp":1698512856370,"user_tz":-540,"elapsed":2342,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MF() :\n","  # hyper_params : 알파나 베타 값 등 딕셔너리로\n","  def __init__(self, ratings, hyper_params) :\n","    # 데이터프레임 형식으로 전달된 평점 넘파이 배열로 바꾸기\n","    self.R = np.array(ratings)\n","    self.num_users, self.num_items = np.shape(self.R)\n","    self.K = hyper_params[\"K\"] # 잠재 요인 개수\n","    self.alpha = hyper_params[\"alpha\"] # 학습률\n","    self.beta = hyper_params[\"beta\"] # 정교화 계수\n","    self.iterations = hyper_params[\"iterations\"] # SGD 얼만큼 반복\n","    self.verbose = hyper_params[\"verbose\"] # 학습 과정 중간에 출력할 것인지 여부를 판단하는 플래그 변수\n","\n","  # P와 Q를 이용해 RMSE를 계산하는 함수\n","  def rmse(self) :\n","    xs, ys = self.R.nonzero() # 0이 아닌 요소의 인덱스 반환\n","    self.predictions = [] # 나중에 prediction과 error를 담을 리스트 변수 초기화\n","    self.errors = []\n","\n","    for x, y in zip(xs, ys) :\n","      prediction = self.get_prediction(x, y) # 사용자 x 아이템 y 에서 평점예측치 계산하는 함수\n","      self.predictions.append(prediction)\n","      # 실제값과 예측값의 차이를 오차값으로 설정\n","      self.errors.append(self.R[x, y] - prediction)\n","    self.predictions = np.array(self.predictions)\n","    self.errors = np.array(self.errors)\n","\n","    return np.sqrt(np.mean(self.errors**2))\n","\n","  # 학습 메소드\n","  def train(self) :\n","    # P와 Q 우선 난수값으로 초기화\n","    # mean을 지정하지 않으면 디폴트로 0\n","    # 표준 편차 sacle을 1/잠재변수개수 로 지정\n","    self.P = np.random.normal(scale = 1./self.K, size = (self.num_users, self.K))\n","    self.Q = np.random.normal(scale = 1./self.K, size = (self.num_items, self.K))\n","\n","    # 사용자 평가 경향\n","    self.b_u = np.zeros(self.num_users)\n","\n","    # 아이템\n","    self.b_d = np.zeros(self.num_items)\n","\n","    # 평점의 전체 평균\n","    self.b = np.mean(self.R[self.R.nonzero()])\n","\n","    # SGD를 적용할 대상 설정\n","    rows, columns = self.R.nonzero()\n","    # 평점의 인덱스와 평점을 리스트로 만들어서 저장\n","    self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n","\n","    # SGD가 한 번 실행될 때마다 RMSE가 얼마나 계산되는지 기록하는 리스트\n","    training_process = []\n","    for i in range(self.iterations) :\n","      # 다양한 시작점에서 SGD 적용\n","      # 데이터의 순서에 따라 모델의 학습 경로가 영향을 받을 수 있기 때문에, 데이터를 무작위로 섞는 것은 중요\n","      np.random.shuffle(self.samples)\n","      self.sgd()\n","      rmse = self.rmse()\n","      training_process.append((i+1, rmse))\n","\n","      # SGD 학습 과정을 중간에 출력할 건지 여부\n","      if self.verbose :\n","        if (i+1) % 10 == 0 :\n","          print(\"Iteration : %d ; train RMSE = %.4f\" %(i+1, rmse))\n","    return training_process\n","\n","  # 평점 예측값 구하는 함수\n","  # 아이템 j에 대한 사용자 i의 평점 예측치\n","  def get_prediction(self, i, j) :\n","    # R_hat\n","    # 전체 평점 + 사용자 평가 경향 + 아이템에 대한 평가 경향 + (사용자 i의 요인 값과 아이템 j 요인의 행렬 연산)\n","    predriction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n","    return predriction\n","\n","  # 최적의 P, Q, B_U, B_D 구하기 위한 과정\n","  def sgd(self) :\n","    for i, j, r in self.samples :\n","      prediction = self.get_prediction(i, j)\n","      # 실제 평점과 비교해 오차 계산\n","      e = (r - prediction)\n","\n","      # 사용자 평가 경향 계산 및 업데이트\n","      self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n","      # 아이템 평가 경향 계산 및 업데이트\n","      self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n","\n","      # 행렬 P 계산 및 업데이트\n","      self.P[i, :] += self.alpha * ((e * self.Q[j, :]) - (self.beta * self.P[i, :]))\n","      # 행렬 Q 계산 및 업데이트\n","      self.Q[j, :] += self.alpha * ((e * self.P[i, :]) - (self.beta * self.Q[j, :]))"],"metadata":{"id":"ZWFPBJqvzoIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","hyper_params = {\n","    \"K\" : 30,\n","    \"alpha\" : 0.001,\n","    \"beta\" : 0.02,\n","    \"iterations\" : 100,\n","    \"verbose\" : True\n","    }\n","mf = MF(R_temp, hyper_params)\n","train_process = mf.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkYA9rJ0Lw6Y","outputId":"1f7c38e9-7b87-4954-87f8-9ed2fd83611d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration : 10 ; train RMSE = 0.9585\n","Iteration : 20 ; train RMSE = 0.9374\n","Iteration : 30 ; train RMSE = 0.9281\n","Iteration : 40 ; train RMSE = 0.9225\n","Iteration : 50 ; train RMSE = 0.9183\n","Iteration : 60 ; train RMSE = 0.9143\n","Iteration : 70 ; train RMSE = 0.9095\n","Iteration : 80 ; train RMSE = 0.9030\n","Iteration : 90 ; train RMSE = 0.8937\n"]}]},{"cell_type":"markdown","source":["## 4.4. train/test 분리 MF 알고리즘"],"metadata":{"id":"8pvkt_hTUhdW"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","\n","base_src = \"drive/MyDrive/RecoSys/python-recosys/Data\"\n","u_data_src = os.path.join(base_src, \"u.data\")\n","r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n","ratings = pd.read_csv(u_data_src, sep = \"\\t\", names = r_cols, encoding = \"latin-1\")\n","ratings = ratings[[\"user_id\", \"movie_id\", \"rating\"]].astype(int)\n","\n","# train/test set 분리\n","## train_test_split을 사용했을 때는 stratify = y로 지정해 계층화 추출을 했었는데\n","## 집단 간 이질성이 크지 않은 경우 오히려 표본의 대표성을 저해할 수 있다\n","## suffle 방식은 완전 무작위\n","TRAIN_SIZE = 0.75\n","\n","## 사용자 - 영화 - 평점\n","## random_state 난수 발생 시드, 시드가 같으면 같은 난수 시퀀스\n","ratings = shuffle(ratings, random_state = 2021)\n","## 데이터 몇 개 뽑을지\n","cutoff = int(TRAIN_SIZE * len(ratings))\n","## iloc : 인덱스로 지정\n","## loc : 레이블로 지정\n","ratings_train = ratings.iloc[:cutoff]\n","ratings_test = ratings.iloc[cutoff:]\n","\n","class NEW_MF() :\n","  def __init__(self, ratings, hyper_params) :\n","    self.R = np.array(ratings)\n","    self.num_users, self.num_items = np.shape(self.R)\n","    # MF weight 조절을 위한 하이퍼 파라미터\n","    ## K : 잠재 요인(latent factor) 의 수\n","    self.K = hyper_params[\"K\"]\n","    ## alpha : 학습률\n","    self.alpha = hyper_params[\"alpha\"]\n","    ## beta : 정규화 계수\n","    self.beta = hyper_params[\"beta\"]\n","    ## iterations : SGD 계산 시 반복 횟수\n","    self.iterations = hyper_params[\"iterations\"]\n","    ## verbose : SGD 학습 과정을 중간중간 출력할 것인지 여부\n","    self.verbose = hyper_params[\"verbose\"]\n","\n","    # 전처리가 잘 이루어진 데이터 셋을 사용하기 때문에 사용자id나 아이템id가 연속된 숫자로 이루어져 있다\n","    # 실제 데이터는 그렇지 않을 수도\n","    # id와 seld.R의 인덱스가 일치하지 않을 수도\n","    ## 아이템 id\n","    item_id_index = []\n","    index_item_id = []\n","    ## 여기에서의 ratings는 full-matrix\n","    ## i는 인덱스 번호, one_id는 movie_id\n","    for i, one_id in enumerate(ratings) :\n","      item_id_index.append([one_id, i])\n","      index_item_id.append([i, one_id])\n","    self.item_id_index = dict(item_id_index)\n","    self.index_item_id = dict(index_item_id)\n","\n","    ## 사용자 id\n","    user_id_index = []\n","    index_user_id = []\n","    for i, one_id in enumerate(ratings.T) :\n","      user_id_index.append([one_id, i])\n","      index_user_id.append([i, one_id])\n","    self.user_id_index = dict(user_id_index)\n","    self.index_user_id = dict(index_user_id)\n","\n","  def rmse(self) :\n","    # self.R에서 평점이 있는 요소의 인덱스를 가져온다\n","    xs, ys = self.R.nonzero()\n","    # prediction과 error를 담을 리스트 변수 초기화\n","    self.predictions = []\n","    self.errors = []\n","\n","    # 평점이 있는 요소(사용자 x, 아이템 y) 각각에 대해 아래 코드 실행\n","    for x, y in zip(xs, ys) :\n","      # 사용자 x, 아이템 y에 대해 평점 예측치를 get_prediction()으로 연산\n","      prediction = self.get_prediction(x, y)\n","      # 예측 리스트에 예측값 추가\n","      self.predictions.append(prediction)\n","      # 실제값 R과 예측값의 차이를 계산해서 오차값 리스트에 추가\n","      error = self.R[x, y] - prediction\n","      self.errors.append(error)\n","    # numpy array 형태로 변환\n","    self.predictions = np.array(self.predictions)\n","    self.errors = np.array(self.errors)\n","\n","    # error 이용해서 rmse 도출\n","    return np.sqrt(np.mean(self.errors ** 2))\n","\n","  def sgd(self) :\n","    for i, j, r in self.samples :\n","      # 사용자 i, 아이템 j에 대한 평점 예측치 계산\n","      prediction = self.get_prediction(i, j)\n","      # 실제 평점과 비교한 오차 계산\n","      e = (r - prediction)\n","\n","      # 사용자 평가 경향 계산 및 업데이트\n","      self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n","      # 아이템 평가 경향 계산 및 업데이트\n","      self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n","\n","      # 행렬 P 계산 및 업데이트\n","      self.P[i, :] += self.alpha * ((e * self.Q[j, :]) - (self.beta * self.P[i, :]))\n","      # 행렬 Q 계산 및 업데이트\n","      self.Q[j, :] += self.alpha * ((e * self.P[i, :]) - (self.beta * self.Q[j, :]))\n","\n","  def get_prediction(self, i, j) :\n","    # 사용자 i, 아이템 j에 대한 예측치\n","    # R_hat\n","    # 전체 평점 + 사용자 평가 경향 + 아이템에 대한 평가 경향 + (사용자 i의 요인 값과 아이템 j 요인의 행렬 연산)\n","    predriction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n","    return predriction\n","\n","  # Test Set 선정\n","  ## 분리된 테스트셋을 넘겨받아서 클래스 내부의 테스트셋을 만드는 함수\n","  def set_test(self, ratings_test) :\n","    test_set = []\n","    for i in range(len(ratings_test)) :\n","      # 실제 테스트 셋에 있는 사용자와 아이템 인덱스, 평점 받아오기\n","      x = self.user_id_index[ratings_test.iloc[i, 0]]\n","      y = self.item_id_index [ratings_test.iloc[i, 1]]\n","      z = ratings_test.iloc[i, 2]\n","\n","      # 테스트 셋 만들기\n","      test_set.append([x,y,z])\n","\n","      # full-matrix에서 테스트 셋이 된 데이터는 지우는 작업\n","      self.R[x,y] = 0\n","\n","    self.test_set = test_set\n","    return test_set\n","\n","  # 테스트 셋 rmse 계산\n","  def test_rmse(self) :\n","    error = 0\n","    for one_set in self.test_set :\n","      predicted = self.get_prediction(one_set[0], one_set[1])\n","      error += pow(one_set[2] - predicted, 2)\n","    return np.sqrt(error/len(self.test_set))\n","\n","  # 학습하면서 테스트 셋의 정확도 계산\n","  def test(self) :\n","    self.P = np.random.normal(scale = 1./self.K, size = (self.num_users, self.K))\n","    self.Q = np.random.normal(scale = 1./self.K, size = (self.num_items, self.K))\n","    self.b_u = np.zeros(self.num_users)\n","    self.b_d = np.zeros(self.num_items)\n","    self.b = np.mean(self.R[self.R.nonzero()])\n","    # 아까 테스트 셋 생성할 때 0으로 바꾸어줘서 지금 여기에서 트레인 셋들의 평균만 얻을 수 있음\n","\n","    # 트레인 셋에 대해 데이터 구성\n","    rows, columns = self.R.nonzero()\n","    self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n","\n","    training_process = []\n","    for i in range(self.iterations) :\n","      np.random.shuffle(self.samples)\n","      self.sgd()\n","      rmse1 = self.rmse()\n","      rmse2 = self.test_rmse()\n","      training_process.append((i+1, rmse1, rmse2))\n","      if self.verbose :\n","        if (i+1) % 10 == 0 :\n","          print(\"Iteration : %d ; Train RMSE = %.4f ; Test RMSE = %.4f\" %(i+1, rmse1, rmse2))\n","\n","    return training_process\n","\n","  # 주어진 사용자와 아이템에 대해 예측치 계산\n","  def get_one_prediction(self, user_id, item_id) :\n","    return self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n","\n","  def full_prediction(self) :\n","    return self.b + self.b_u[:, np.newaxis] + self.b_d[np.newaxis, :] + self.P.dot(self.Q.T)\n","    # np.newaxis 행렬 연산하기 위해"],"metadata":{"id":"cILh9y8oMmt8","executionInfo":{"status":"ok","timestamp":1698517357605,"user_tz":-540,"elapsed":569,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","hyper_params = {\n","    \"K\" : 30,\n","    \"alpha\" : 0.001,\n","    \"beta\" : 0.02,\n","    \"iterations\" : 100,\n","    \"verbose\" : True\n","}\n","mf = NEW_MF(R_temp, hyper_params)\n","test_set = mf.set_test(ratings_test)\n","result = mf.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gASphdweQLBp","executionInfo":{"status":"ok","timestamp":1698517603793,"user_tz":-540,"elapsed":244769,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"d823b026-a0ae-4d14-929a-ab81c9a76c13"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration : 10 ; Train RMSE = 0.9666 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9412 ; Test RMSE = 0.9623\n","Iteration : 30 ; Train RMSE = 0.9297 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9228 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9180 ; Test RMSE = 0.9493\n","Iteration : 60 ; Train RMSE = 0.9139 ; Test RMSE = 0.9477\n","Iteration : 70 ; Train RMSE = 0.9101 ; Test RMSE = 0.9465\n","Iteration : 80 ; Train RMSE = 0.9058 ; Test RMSE = 0.9453\n","Iteration : 90 ; Train RMSE = 0.9006 ; Test RMSE = 0.9439\n","Iteration : 100 ; Train RMSE = 0.8938 ; Test RMSE = 0.9420\n"]}]},{"cell_type":"code","source":["print(mf.full_prediction())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v76HdUugQJTg","executionInfo":{"status":"ok","timestamp":1698517606139,"user_tz":-540,"elapsed":5,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"cb6ac394-0782-43e4-b924-e28facac77de"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3.93426353 3.35796021 3.04743957 ... 3.36392084 3.48930165 3.43055794]\n"," [3.76968056 3.29835256 2.88793883 ... 3.26575961 3.38510305 3.35789363]\n"," [3.48029515 2.8665424  2.5139247  ... 2.88276482 2.963549   2.96522613]\n"," ...\n"," [4.14559824 3.61757015 3.22183201 ... 3.58057786 3.69944908 3.67553149]\n"," [4.30617612 3.78197372 3.37488704 ... 3.75236357 3.88301913 3.85706329]\n"," [3.81998855 3.37180497 2.90931553 ... 3.28279835 3.41449877 3.40790501]]\n"]}]},{"cell_type":"code","source":["print(mf.get_one_prediction(1, 2))\n","# 사용자 1이 아이템 2에 대한 예측"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fybz6YWhto1","executionInfo":{"status":"ok","timestamp":1698517606140,"user_tz":-540,"elapsed":3,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"055b55b4-bca2-49a1-fde2-86968f7d94be"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["3.3579602107076827\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SQqwUhBWh7VV"},"execution_count":null,"outputs":[]}]}