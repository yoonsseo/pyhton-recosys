{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1wpqCzPIa9Xf-nZgTPTyvjZSsU2cqw333","authorship_tag":"ABX9TyPQ+1Cmuxf/ltgC+EA+VvPL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#4. Matrix Factorization(MF) 기반 추천\n","\n","||메모리 기반 알고리즘|모델 기반 알고리즘|\n","|---|---|---|\n","|설명|메모리에 있는 데이터를 계산해서 <br> 추천하는 방식|데이터로부터 미리 모델을 구성 후, <br> 필요 시 추천하는 방식|\n","|특징|개별 사용자 데이터 집중|전체 사용자 패턴 집중|\n","|장점|원래 데이터에 충실하게 사용|대규모 데이터에 빠르게 반응|\n","|단점|대규모 데이터에 느리게 반응|모델 생성 과정 오래 걸림|\n","|예시|CF 기반 추천 알고리즘|MF 기반 추천 알고리즘, 딥러닝|"],"metadata":{"id":"xwXDeRu_AXBe"}},{"cell_type":"markdown","source":["## 4.1. Matrix Factorization(MF) 방식의 원리\n","* Matrix Factorization : 행렬 요인화/분해\n","* 평가, 사용자, 아이템으로 구성된 하나의 행렬을, 두 개의 행렬로 분해\n","* R ≈ P × Q.T = R^ (예상 평점)\n","  * Rating matrix R - M×N 차원\n","  * User latent matrix 사용자와 사용자 잠재 요인 행렬 P - M×K 차원\n","  * Item latent matrix 아이템과 아이템 잠재 요인 행렬 Q - N×K 차원\n","* CF에서는 사용자와 아이템, 평점으로 이루어진 full-matrix R 이용"],"metadata":{"id":"LW3hwHy2CIOb"}},{"cell_type":"markdown","source":["## 4.2. SGD(Stochastic Gradient Decent)를 사용한 MF 알고리즘\n"," * SGD를 사용해 MF의 P와 Q 행렬을 구하는 게 최종 목표  \n","\n","### 4.2.1. MF 알고리즘 개념적 설명\n","> 1. 잠재 요인 개수 K 선택\n","> 2. P, Q 행렬 초기화  \n",">\n","> [ 반복 ]  \n","> > 3. 예측 평점 R_hat(= P×Q.T) 계산\n","> > 4. 실제 R과 R_hat 간 오차 계산 및 P, Q 수정 (오차 감소 위함)\n","> > → 가장 중요한 단계\n","> > 5. 기준 오차 도달 확인\n","\n","* MF의 핵심 : P와 Q 잘 분해하기\n","  * 주어진 사용자와 아이템의 관계를 잘 설명할 수 있도록  \n","\n","### 4.2.2. SGD : Stochastic Gradient Decent\n","* 예측 오차를 줄이기위한 P, Q 업데이트\n","  * 예측 오차 제곱의 편미분 값 사용\n","  * 학습률(learning rate) α 알파 활용\n","* Overfitting 과적합 방지\n","  * 정규화 고려\n","    * 정규화 항(Regulation term) 추가\n","    * 정교화 계수 β\n","  * 경향성 고려\n","    * 사용자와 아이템의 경향성 문제\n","      * 전체 평균 b\n","      * 전체 평균을 제거한 후 사용자 i의 평가 경향 bu[i]\n","        * 사용자 i 평균과 전체 평균의 차이\n","      * 전체 평균을 제거한 후 아이템 j의 평가 경향 bi[j]\n","        * 아이템 j의 평균과 전체 평균의 차이\n","    * CF에서는 사용자와 아이템 별로 평가 경향이 한 번에 계산되었는데  \n","    MF에서는 계산할 때마다 오차를 최소화하도록 bu[i]와 bi[j] 계속 업데이트\n","\n","\n"],"metadata":{"id":"IUuvAt6XLB7Y"}},{"cell_type":"markdown","source":["## 4.3. SGD를 사용한 MF 기본 알고리즘"],"metadata":{"id":"UToSmRj_cb18"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","base_src = \"drive/MyDrive/RecoSys/python-recosys/Data\"\n","u_data_src = os.path.join(base_src, \"u.data\")\n","r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n","ratings = pd.read_csv(u_data_src, sep = \"\\t\", names = r_cols, encoding = \"latin-1\")\n","ratings = ratings[[\"user_id\", \"movie_id\", \"rating\"]].astype(int)"],"metadata":{"id":"5BoxonY5cZyz","executionInfo":{"status":"ok","timestamp":1698488756629,"user_tz":-540,"elapsed":5,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class MF() :\n","  # hyper_params : 알파나 베타 값 등 딕셔너리로\n","  def __init__(self, ratings, hyper_params) :\n","    # 데이터프레임 형식으로 전달된 평점 넘파이 배열로 바꾸기\n","    self.R = np.array(ratings)\n","    self.num_users, self.num_items = np.shape(self.R)\n","    self.K = hyper_params[\"K\"] # 잠재 요인 개수\n","    self.alpha = hyper_params[\"alpha\"] # 학습률\n","    self.beta = hyper_params[\"beta\"] # 정교화 계수\n","    self.iterations = hyper_params[\"iterations\"] # SGD 얼만큼 반복\n","    self.verbose = hyper_params[\"verbose\"] # 학습 과정 중간에 출력할 것인지 여부를 판단하는 플래그 변수\n","\n","  # P와 Q를 이용해 RMSE를 계산하는 함수\n","  def rmse(self) :\n","    xs, ys = self.R.nonzero() # 0이 아닌 요소의 인덱스 반환\n","    self.predictions = [] # 나중에 prediction과 error를 담을 리스트 변수 초기화\n","    self.errors = []\n","\n","    for x, y in zip(xs, ys) :\n","      prediction = self.get_prediction(x, y) # 사용자 x 아이템 y 에서 평점예측치 계산하는 함수\n","      self.predictions.append(prediction)\n","      # 실제값과 예측값의 차이를 오차값으로 설정\n","      self.errors.append(self.R[x, y] - prediction)\n","    self.predictions = np.array(self.predictions)\n","    self.errors = np.array(self.errors)\n","\n","    return np.sqrt(np.mean(self.errors**2))\n","\n","  # 학습 메소드\n","  def train(self) :\n","    # P와 Q 우선 난수값으로 초기화\n","    # mean을 지정하지 않으면 디폴트로 0\n","    # 표준 편차 sacle을 1/잠재변수개수 로 지정\n","    self.P = np.random.normal(scale = 1./self.K, size = (self.num_users, self.K))\n","    self.Q = np.random.normal(scale = 1./self.K, size = (self.num_items, self.K))\n","\n","    # 사용자 평가 경향\n","    self.b_u = np.zeros(self.num_users)\n","\n","    # 아이템\n","    self.b_d = np.zeros(self.num_items)\n","\n","    # 평점의 전체 평균\n","    self.b = np.mean(self.R[self.R.nonzero()])\n","\n","    # SGD를 적용할 대상 설정\n","    rows, columns = self.R.nonzero()\n","    # 평점의 인덱스와 평점을 리스트로 만들어서 저장\n","    self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n","\n","    # SGD가 한 번 실행될 때마다 RMSE가 얼마나 계산되는지 기록하는 리스트\n","    training_process = []\n","    for i in range(self.iterations) :\n","      # 다양한 시작점에서 SGD 적용\n","      # 데이터의 순서에 따라 모델의 학습 경로가 영향을 받을 수 있기 때문에, 데이터를 무작위로 섞는 것은 중요\n","      np.random.shuffle(self.samples)\n","      self.sgd()\n","      rmse = self.rmse()\n","      training_process.append((i+1, rmse))\n","\n","      # SGD 학습 과정을 중간에 출력할 건지 여부\n","      if self.verbose :\n","        if (i+1) % 10 == 0 :\n","          print(\"Iteration : %d ; train RMSE = %.4f\" %(i+1, rmse))\n","    return training_process\n","\n","  # 평점 예측값 구하는 함수\n","  # 아이템 j에 대한 사용자 i의 평점 예측치\n","  def get_prediction(self, i, j) :\n","    # R_hat\n","    # 전체 평점 + 사용자 평가 경향 + 아이템에 대한 평가 경향 + (사용자 i의 요인 값과 아이템 j 요인의 행렬 연산)\n","    predriction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n","    return predriction\n","\n","  # 최적의 P, Q, B_U, B_D 구하기 위한 과정\n","  def sgd(self) :\n","    for i, j, r in self.samples :\n","      prediction = self.get_prediction(i, j)\n","      # 실제 평점과 비교해 오차 계산\n","      e = (r - prediction)\n","\n","      # 사용자 평가 경향 계산 및 업데이트\n","      self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n","      # 아이템 평가 경향 계산 및 업데이트\n","      self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n","\n","      # 행렬 P 계산 및 업데이트\n","      self.P[i, :] += self.alpha * ((e * self.Q[j, :]) - (self.beta * self.P[i, :]))\n","      # 행렬 Q 계산 및 업데이트\n","      self.Q[j, :] += self.alpha * ((e * self.P[i, :]) - (self.beta * self.Q[j, :]))"],"metadata":{"id":"ZWFPBJqvzoIx","executionInfo":{"status":"ok","timestamp":1698495141032,"user_tz":-540,"elapsed":500,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","hyper_params = {\n","    \"K\" : 30,\n","    \"alpha\" : 0.001,\n","    \"beta\" : 0.02,\n","    \"iterations\" : 100,\n","    \"verbose\" : True\n","    }\n","mf = MF(R_temp, hyper_params)\n","train_process = mf.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkYA9rJ0Lw6Y","outputId":"1f7c38e9-7b87-4954-87f8-9ed2fd83611d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration : 10 ; train RMSE = 0.9585\n","Iteration : 20 ; train RMSE = 0.9374\n","Iteration : 30 ; train RMSE = 0.9281\n","Iteration : 40 ; train RMSE = 0.9225\n","Iteration : 50 ; train RMSE = 0.9183\n","Iteration : 60 ; train RMSE = 0.9143\n","Iteration : 70 ; train RMSE = 0.9095\n","Iteration : 80 ; train RMSE = 0.9030\n","Iteration : 90 ; train RMSE = 0.8937\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cILh9y8oMmt8"},"execution_count":null,"outputs":[]}]}