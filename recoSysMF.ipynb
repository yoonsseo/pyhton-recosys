{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1wpqCzPIa9Xf-nZgTPTyvjZSsU2cqw333","authorship_tag":"ABX9TyMHU7/59XRhtUFhSpr9yJ2K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#4. Matrix Factorization(MF) 기반 추천\n","\n","||메모리 기반 알고리즘|모델 기반 알고리즘|\n","|---|---|---|\n","|설명|메모리에 있는 데이터를 계산해서 <br> 추천하는 방식|데이터로부터 미리 모델을 구성 후, <br> 필요 시 추천하는 방식|\n","|특징|개별 사용자 데이터 집중|전체 사용자 패턴 집중|\n","|장점|원래 데이터에 충실하게 사용|대규모 데이터에 빠르게 반응|\n","|단점|대규모 데이터에 느리게 반응|모델 생성 과정 오래 걸림|\n","|예시|CF 기반 추천 알고리즘|MF 기반 추천 알고리즘, 딥러닝|"],"metadata":{"id":"xwXDeRu_AXBe"}},{"cell_type":"markdown","source":["## 4.1. Matrix Factorization(MF) 방식의 원리\n","* Matrix Factorization : 행렬 요인화/분해\n","* 평가, 사용자, 아이템으로 구성된 하나의 행렬을, 두 개의 행렬로 분해\n","* R ≈ P × Q.T = R^ (예상 평점)\n","  * Rating matrix R - M×N 차원\n","  * User latent matrix 사용자와 사용자 잠재 요인 행렬 P - M×K 차원\n","  * Item latent matrix 아이템과 아이템 잠재 요인 행렬 Q - N×K 차원\n","* CF에서는 사용자와 아이템, 평점으로 이루어진 full-matrix R 이용"],"metadata":{"id":"LW3hwHy2CIOb"}},{"cell_type":"markdown","source":["## 4.2. SGD(Stochastic Gradient Decent)를 사용한 MF 알고리즘\n"," * SGD를 사용해 MF의 P와 Q 행렬을 구하는 게 최종 목표  \n","\n","### 4.2.1. MF 알고리즘 개념적 설명\n","> 1. 잠재 요인 개수 K 선택\n","> 2. P, Q 행렬 초기화  \n",">\n","> [ 반복 ]  \n","> > 3. 예측 평점 R_hat(= P×Q.T) 계산\n","> > 4. 실제 R과 R_hat 간 오차 계산 및 P, Q 수정 (오차 감소 위함)\n","> > → 가장 중요한 단계\n","> > 5. 기준 오차 도달 확인\n","\n","* MF의 핵심 : P와 Q 잘 분해하기\n","  * 주어진 사용자와 아이템의 관계를 잘 설명할 수 있도록  \n","\n","### 4.2.2. SGD : Stochastic Gradient Decent\n","* 예측 오차를 줄이기위한 P, Q 업데이트\n","  * 예측 오차 제곱의 편미분 값 사용\n","  * 학습률(learning rate) α 알파 활용\n","* Overfitting 과적합 방지\n","  * 정규화 고려\n","    * 정규화 항(Regulation term) 추가\n","    * 정교화 계수 β\n","  * 경향성 고려\n","    * 사용자와 아이템의 경향성 문제\n","      * 전체 평균 b\n","      * 전체 평균을 제거한 후 사용자 i의 평가 경향 bu[i]\n","        * 사용자 i 평균과 전체 평균의 차이\n","      * 전체 평균을 제거한 후 아이템 j의 평가 경향 bi[j]\n","        * 아이템 j의 평균과 전체 평균의 차이\n","    * CF에서는 사용자와 아이템 별로 평가 경향이 한 번에 계산되었는데  \n","    MF에서는 계산할 때마다 오차를 최소화하도록 bu[i]와 bi[j] 계속 업데이트\n","\n","\n"],"metadata":{"id":"IUuvAt6XLB7Y"}},{"cell_type":"markdown","source":["## 4.3. SGD를 사용한 MF 기본 알고리즘"],"metadata":{"id":"UToSmRj_cb18"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","base_src = \"drive/MyDrive/RecoSys/python-recosys/Data\"\n","u_data_src = os.path.join(base_src, \"u.data\")\n","r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n","ratings = pd.read_csv(u_data_src, sep = \"\\t\", names = r_cols, encoding = \"latin-1\")\n","ratings = ratings[[\"user_id\", \"movie_id\", \"rating\"]].astype(int)"],"metadata":{"id":"5BoxonY5cZyz","executionInfo":{"status":"ok","timestamp":1698854578071,"user_tz":-540,"elapsed":2042,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class MF() :\n","  # hyper_params : 알파나 베타 값 등 딕셔너리로\n","  def __init__(self, ratings, hyper_params) :\n","    # 데이터프레임 형식으로 전달된 평점 넘파이 배열로 바꾸기\n","    self.R = np.array(ratings)\n","    self.num_users, self.num_items = np.shape(self.R)\n","    self.K = hyper_params[\"K\"] # 잠재 요인 개수\n","    self.alpha = hyper_params[\"alpha\"] # 학습률\n","    self.beta = hyper_params[\"beta\"] # 정교화 계수\n","    self.iterations = hyper_params[\"iterations\"] # SGD 얼만큼 반복\n","    self.verbose = hyper_params[\"verbose\"] # 학습 과정 중간에 출력할 것인지 여부를 판단하는 플래그 변수\n","\n","  # P와 Q를 이용해 RMSE를 계산하는 함수\n","  def rmse(self) :\n","    xs, ys = self.R.nonzero() # 0이 아닌 요소의 인덱스 반환\n","    self.predictions = [] # 나중에 prediction과 error를 담을 리스트 변수 초기화\n","    self.errors = []\n","\n","    for x, y in zip(xs, ys) :\n","      prediction = self.get_prediction(x, y) # 사용자 x 아이템 y 에서 평점예측치 계산하는 함수\n","      self.predictions.append(prediction)\n","      # 실제값과 예측값의 차이를 오차값으로 설정\n","      self.errors.append(self.R[x, y] - prediction)\n","    self.predictions = np.array(self.predictions)\n","    self.errors = np.array(self.errors)\n","\n","    return np.sqrt(np.mean(self.errors**2))\n","\n","  # 학습 메소드\n","  def train(self) :\n","    # P와 Q 우선 난수값으로 초기화\n","    # mean을 지정하지 않으면 디폴트로 0\n","    # 표준 편차 sacle을 1/잠재변수개수 로 지정\n","    self.P = np.random.normal(scale = 1./self.K, size = (self.num_users, self.K))\n","    self.Q = np.random.normal(scale = 1./self.K, size = (self.num_items, self.K))\n","\n","    # 사용자 평가 경향\n","    self.b_u = np.zeros(self.num_users)\n","\n","    # 아이템\n","    self.b_d = np.zeros(self.num_items)\n","\n","    # 평점의 전체 평균\n","    self.b = np.mean(self.R[self.R.nonzero()])\n","\n","    # SGD를 적용할 대상 설정\n","    rows, columns = self.R.nonzero()\n","    # 평점의 인덱스와 평점을 리스트로 만들어서 저장\n","    self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n","\n","    # SGD가 한 번 실행될 때마다 RMSE가 얼마나 계산되는지 기록하는 리스트\n","    training_process = []\n","    for i in range(self.iterations) :\n","      # 다양한 시작점에서 SGD 적용\n","      # 데이터의 순서에 따라 모델의 학습 경로가 영향을 받을 수 있기 때문에, 데이터를 무작위로 섞는 것은 중요\n","      np.random.shuffle(self.samples)\n","      self.sgd()\n","      rmse = self.rmse()\n","      training_process.append((i+1, rmse))\n","\n","      # SGD 학습 과정을 중간에 출력할 건지 여부\n","      if self.verbose :\n","        if (i+1) % 10 == 0 :\n","          print(\"Iteration : %d ; train RMSE = %.4f\" %(i+1, rmse))\n","    return training_process\n","\n","  # 평점 예측값 구하는 함수\n","  # 아이템 j에 대한 사용자 i의 평점 예측치\n","  def get_prediction(self, i, j) :\n","    # R_hat\n","    # 전체 평점 + 사용자 평가 경향 + 아이템에 대한 평가 경향 + (사용자 i의 요인 값과 아이템 j 요인의 행렬 연산)\n","    predriction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n","    return predriction\n","\n","  # 최적의 P, Q, B_U, B_D 구하기 위한 과정\n","  def sgd(self) :\n","    for i, j, r in self.samples :\n","      prediction = self.get_prediction(i, j)\n","      # 실제 평점과 비교해 오차 계산\n","      e = (r - prediction)\n","\n","      # 사용자 평가 경향 계산 및 업데이트\n","      self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n","      # 아이템 평가 경향 계산 및 업데이트\n","      self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n","\n","      # 행렬 P 계산 및 업데이트\n","      self.P[i, :] += self.alpha * ((e * self.Q[j, :]) - (self.beta * self.P[i, :]))\n","      # 행렬 Q 계산 및 업데이트\n","      self.Q[j, :] += self.alpha * ((e * self.P[i, :]) - (self.beta * self.Q[j, :]))"],"metadata":{"id":"ZWFPBJqvzoIx","executionInfo":{"status":"ok","timestamp":1698854581346,"user_tz":-540,"elapsed":6,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","hyper_params = {\n","    \"K\" : 30,\n","    \"alpha\" : 0.001,\n","    \"beta\" : 0.02,\n","    \"iterations\" : 100,\n","    \"verbose\" : True\n","    }\n","mf = MF(R_temp, hyper_params)\n","train_process = mf.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkYA9rJ0Lw6Y","outputId":"1b55b5bf-f62d-4990-8979-2e229bd08aad","executionInfo":{"status":"ok","timestamp":1698854946719,"user_tz":-540,"elapsed":358461,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration : 10 ; train RMSE = 0.9585\n","Iteration : 20 ; train RMSE = 0.9374\n","Iteration : 30 ; train RMSE = 0.9281\n","Iteration : 40 ; train RMSE = 0.9225\n","Iteration : 50 ; train RMSE = 0.9184\n","Iteration : 60 ; train RMSE = 0.9145\n","Iteration : 70 ; train RMSE = 0.9099\n","Iteration : 80 ; train RMSE = 0.9038\n","Iteration : 90 ; train RMSE = 0.8949\n","Iteration : 100 ; train RMSE = 0.8831\n"]}]},{"cell_type":"markdown","source":["## 4.4. train/test 분리 MF 알고리즘"],"metadata":{"id":"8pvkt_hTUhdW"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","\n","base_src = \"drive/MyDrive/RecoSys/python-recosys/Data\"\n","u_data_src = os.path.join(base_src, \"u.data\")\n","r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n","ratings = pd.read_csv(u_data_src, sep = \"\\t\", names = r_cols, encoding = \"latin-1\")\n","ratings = ratings[[\"user_id\", \"movie_id\", \"rating\"]].astype(int)\n","\n","# train/test set 분리\n","## train_test_split을 사용했을 때는 stratify = y로 지정해 계층화 추출을 했었는데\n","## 집단 간 이질성이 크지 않은 경우 오히려 표본의 대표성을 저해할 수 있다\n","## suffle 방식은 완전 무작위\n","TRAIN_SIZE = 0.75\n","\n","## 사용자 - 영화 - 평점\n","## random_state 난수 발생 시드, 시드가 같으면 같은 난수 시퀀스\n","ratings = shuffle(ratings, random_state = 2021)\n","## 데이터 몇 개 뽑을지\n","cutoff = int(TRAIN_SIZE * len(ratings))\n","## iloc : 인덱스로 지정\n","## loc : 레이블로 지정\n","ratings_train = ratings.iloc[:cutoff]\n","ratings_test = ratings.iloc[cutoff:]\n","\n","class NEW_MF() :\n","  def __init__(self, ratings, hyper_params) :\n","    self.R = np.array(ratings)\n","    self.num_users, self.num_items = np.shape(self.R)\n","    # MF weight 조절을 위한 하이퍼 파라미터\n","    ## K : 잠재 요인(latent factor) 의 수\n","    self.K = hyper_params[\"K\"]\n","    ## alpha : 학습률\n","    self.alpha = hyper_params[\"alpha\"]\n","    ## beta : 정규화 계수\n","    self.beta = hyper_params[\"beta\"]\n","    ## iterations : SGD 계산 시 반복 횟수\n","    self.iterations = hyper_params[\"iterations\"]\n","    ## verbose : SGD 학습 과정을 중간중간 출력할 것인지 여부\n","    self.verbose = hyper_params[\"verbose\"]\n","\n","    # 전처리가 잘 이루어진 데이터 셋을 사용하기 때문에 사용자id나 아이템id가 연속된 숫자로 이루어져 있다\n","    # 실제 데이터는 그렇지 않을 수도\n","    # id와 seld.R의 인덱스가 일치하지 않을 수도\n","    ## 아이템 id\n","    item_id_index = []\n","    index_item_id = []\n","    ## 여기에서의 ratings는 full-matrix\n","    ## i는 인덱스 번호, one_id는 movie_id\n","    for i, one_id in enumerate(ratings) :\n","      item_id_index.append([one_id, i])\n","      index_item_id.append([i, one_id])\n","    self.item_id_index = dict(item_id_index)\n","    self.index_item_id = dict(index_item_id)\n","\n","    ## 사용자 id\n","    user_id_index = []\n","    index_user_id = []\n","    for i, one_id in enumerate(ratings.T) :\n","      user_id_index.append([one_id, i])\n","      index_user_id.append([i, one_id])\n","    self.user_id_index = dict(user_id_index)\n","    self.index_user_id = dict(index_user_id)\n","\n","  def rmse(self) :\n","    # self.R에서 평점이 있는 요소의 인덱스를 가져온다\n","    xs, ys = self.R.nonzero()\n","    # prediction과 error를 담을 리스트 변수 초기화\n","    self.predictions = []\n","    self.errors = []\n","\n","    # 평점이 있는 요소(사용자 x, 아이템 y) 각각에 대해 아래 코드 실행\n","    for x, y in zip(xs, ys) :\n","      # 사용자 x, 아이템 y에 대해 평점 예측치를 get_prediction()으로 연산\n","      prediction = self.get_prediction(x, y)\n","      # 예측 리스트에 예측값 추가\n","      self.predictions.append(prediction)\n","      # 실제값 R과 예측값의 차이를 계산해서 오차값 리스트에 추가\n","      error = self.R[x, y] - prediction\n","      self.errors.append(error)\n","    # numpy array 형태로 변환\n","    self.predictions = np.array(self.predictions)\n","    self.errors = np.array(self.errors)\n","\n","    # error 이용해서 rmse 도출\n","    return np.sqrt(np.mean(self.errors ** 2))\n","\n","  def sgd(self) :\n","    for i, j, r in self.samples :\n","      # 사용자 i, 아이템 j에 대한 평점 예측치 계산\n","      prediction = self.get_prediction(i, j)\n","      # 실제 평점과 비교한 오차 계산\n","      e = (r - prediction)\n","\n","      # 사용자 평가 경향 계산 및 업데이트\n","      self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n","      # 아이템 평가 경향 계산 및 업데이트\n","      self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n","\n","      # 행렬 P 계산 및 업데이트\n","      self.P[i, :] += self.alpha * ((e * self.Q[j, :]) - (self.beta * self.P[i, :]))\n","      # 행렬 Q 계산 및 업데이트\n","      self.Q[j, :] += self.alpha * ((e * self.P[i, :]) - (self.beta * self.Q[j, :]))\n","\n","  def get_prediction(self, i, j) :\n","    # 사용자 i, 아이템 j에 대한 예측치\n","    # R_hat\n","    # 전체 평점 + 사용자 평가 경향 + 아이템에 대한 평가 경향 + (사용자 i의 요인 값과 아이템 j 요인의 행렬 연산)\n","    predriction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n","    return predriction\n","\n","  # Test Set 선정\n","  ## 분리된 테스트셋을 넘겨받아서 클래스 내부의 테스트셋을 만드는 함수\n","  def set_test(self, ratings_test) :\n","    test_set = []\n","    for i in range(len(ratings_test)) :\n","      # 실제 테스트 셋에 있는 사용자와 아이템 인덱스, 평점 받아오기\n","      x = self.user_id_index[ratings_test.iloc[i, 0]]\n","      y = self.item_id_index [ratings_test.iloc[i, 1]]\n","      z = ratings_test.iloc[i, 2]\n","\n","      # 테스트 셋 만들기\n","      test_set.append([x,y,z])\n","\n","      # full-matrix에서 테스트 셋이 된 데이터는 지우는 작업\n","      self.R[x,y] = 0\n","\n","    self.test_set = test_set\n","    return test_set\n","\n","  # 테스트 셋 rmse 계산\n","  def test_rmse(self) :\n","    error = 0\n","    for one_set in self.test_set :\n","      predicted = self.get_prediction(one_set[0], one_set[1])\n","      error += pow(one_set[2] - predicted, 2)\n","    return np.sqrt(error/len(self.test_set))\n","\n","  # 학습하면서 테스트 셋의 정확도 계산\n","  def test(self) :\n","    self.P = np.random.normal(scale = 1./self.K, size = (self.num_users, self.K))\n","    self.Q = np.random.normal(scale = 1./self.K, size = (self.num_items, self.K))\n","    self.b_u = np.zeros(self.num_users)\n","    self.b_d = np.zeros(self.num_items)\n","    self.b = np.mean(self.R[self.R.nonzero()])\n","    # 아까 테스트 셋 생성할 때 0으로 바꾸어줘서 지금 여기에서 트레인 셋들의 평균만 얻을 수 있음\n","\n","    # 트레인 셋에 대해 데이터 구성\n","    rows, columns = self.R.nonzero()\n","    self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n","\n","    training_process = []\n","    for i in range(self.iterations) :\n","      np.random.shuffle(self.samples)\n","      self.sgd()\n","      rmse1 = self.rmse()\n","      rmse2 = self.test_rmse()\n","      training_process.append((i+1, rmse1, rmse2))\n","      if self.verbose :\n","        if (i+1) % 10 == 0 :\n","          print(\"Iteration : %d ; Train RMSE = %.4f ; Test RMSE = %.4f\" %(i+1, rmse1, rmse2))\n","\n","    return training_process\n","\n","  # 주어진 사용자와 아이템에 대해 예측치 계산\n","  def get_one_prediction(self, user_id, item_id) :\n","    return self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n","\n","  def full_prediction(self) :\n","    return self.b + self.b_u[:, np.newaxis] + self.b_d[np.newaxis, :] + self.P.dot(self.Q.T)\n","    # np.newaxis 행렬 연산하기 위해"],"metadata":{"id":"cILh9y8oMmt8","executionInfo":{"status":"ok","timestamp":1698888495520,"user_tz":-540,"elapsed":2315,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","hyper_params = {\n","    \"K\" : 30,\n","    \"alpha\" : 0.001,\n","    \"beta\" : 0.02,\n","    \"iterations\" : 100,\n","    \"verbose\" : True\n","}\n","mf = NEW_MF(R_temp, hyper_params)\n","test_set = mf.set_test(ratings_test)\n","result = mf.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gASphdweQLBp","executionInfo":{"status":"ok","timestamp":1698888723988,"user_tz":-540,"elapsed":228471,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"7435e9e0-6b0a-45de-b86b-8fc090e4a43d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration : 10 ; Train RMSE = 0.9666 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9412 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9297 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9228 ; Test RMSE = 0.9514\n","Iteration : 50 ; Train RMSE = 0.9179 ; Test RMSE = 0.9491\n","Iteration : 60 ; Train RMSE = 0.9138 ; Test RMSE = 0.9476\n","Iteration : 70 ; Train RMSE = 0.9099 ; Test RMSE = 0.9463\n","Iteration : 80 ; Train RMSE = 0.9057 ; Test RMSE = 0.9451\n","Iteration : 90 ; Train RMSE = 0.9004 ; Test RMSE = 0.9436\n","Iteration : 100 ; Train RMSE = 0.8936 ; Test RMSE = 0.9417\n"]}]},{"cell_type":"code","source":["print(mf.full_prediction())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v76HdUugQJTg","executionInfo":{"status":"ok","timestamp":1698888723989,"user_tz":-540,"elapsed":14,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"4e25be70-c904-4b66-d6d8-339be848da1d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3.87980178 3.34338145 2.96055887 ... 3.34092972 3.48263425 3.45461261]\n"," [3.8244984  3.27396861 2.91573086 ... 3.25672117 3.3663734  3.35885352]\n"," [3.43717231 2.9087714  2.51649303 ... 2.8598944  2.98732412 2.95053395]\n"," ...\n"," [4.14928322 3.61034908 3.23668372 ... 3.58728544 3.70442883 3.68045791]\n"," [4.30757931 3.78933879 3.41166885 ... 3.74856465 3.87458828 3.85804816]\n"," [3.86248011 3.33107724 2.91803774 ... 3.3005338  3.38639646 3.3819701 ]]\n"]}]},{"cell_type":"code","source":["print(mf.get_one_prediction(1, 2))\n","# 사용자 1이 아이템 2에 대한 예측"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fybz6YWhto1","executionInfo":{"status":"ok","timestamp":1698888723989,"user_tz":-540,"elapsed":11,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"aae7d16a-2357-476b-92eb-b34df9fde24f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["3.3433814480640756\n"]}]},{"cell_type":"markdown","source":["## 4.5. MF 최적 파라미터 찾기\n","> 1. 대략적인 최적의 K 위치 찾기\n","> 2. 대략적 K 주변 탐색으로, 최적 K 찾기\n","> 3. 주어진 K 통해 최적의 iterations 선\n","\n","* K외에도 다양한 파라미터 alpha, beta 등등에 대해\n","  * 개개별 최적의 파라미터를 구했다 할 지라도\n","  * 이들의 단순 조합이 최고의 결과를 낸다는 보장은 없음\n","  "],"metadata":{"id":"6sW6tibhJ73k"}},{"cell_type":"code","source":["# 최적의 k 찾기\n","\n","## 성능에 대한 결과값 담기\n","results = []\n","## 어떤 K일 때 어떤 결과가 나왔는지 나타내기 위함\n","index = []\n","\n","R_temp = ratings.pivot(index = \"user_id\", columns = \"movie_id\", values = \"rating\").fillna(0)\n","\n","for K in range(50, 261, 20) :\n","  print(f\"K : {K}\")\n","  hyper_params = {\n","      \"K\" : K,\n","      \"alpha\" : 0.001,\n","      \"beta\" : 0.02,\n","      \"iterations\" : 300,\n","      \"verbose\" : True\n","  }\n","  mf = NEW_MF(R_temp, hyper_params)\n","  test_set = mf.set_test(ratings_test)\n","  result = mf.test()\n","  index.append(K)\n","  results.append(result)"],"metadata":{"id":"SQqwUhBWh7VV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698897117441,"user_tz":-540,"elapsed":8393462,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"01cfbb10-423a-4ada-e52b-4f4b655418f0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["K : 50\n","Iteration : 10 ; Train RMSE = 0.9669 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9417 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9305 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9240 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9195 ; Test RMSE = 0.9493\n","Iteration : 60 ; Train RMSE = 0.9161 ; Test RMSE = 0.9479\n","Iteration : 70 ; Train RMSE = 0.9132 ; Test RMSE = 0.9469\n","Iteration : 80 ; Train RMSE = 0.9102 ; Test RMSE = 0.9460\n","Iteration : 90 ; Train RMSE = 0.9069 ; Test RMSE = 0.9452\n","Iteration : 100 ; Train RMSE = 0.9027 ; Test RMSE = 0.9441\n","Iteration : 110 ; Train RMSE = 0.8970 ; Test RMSE = 0.9426\n","Iteration : 120 ; Train RMSE = 0.8893 ; Test RMSE = 0.9405\n","Iteration : 130 ; Train RMSE = 0.8791 ; Test RMSE = 0.9376\n","Iteration : 140 ; Train RMSE = 0.8664 ; Test RMSE = 0.9342\n","Iteration : 150 ; Train RMSE = 0.8516 ; Test RMSE = 0.9305\n","Iteration : 160 ; Train RMSE = 0.8351 ; Test RMSE = 0.9272\n","Iteration : 170 ; Train RMSE = 0.8174 ; Test RMSE = 0.9244\n","Iteration : 180 ; Train RMSE = 0.7986 ; Test RMSE = 0.9221\n","Iteration : 190 ; Train RMSE = 0.7787 ; Test RMSE = 0.9205\n","Iteration : 200 ; Train RMSE = 0.7579 ; Test RMSE = 0.9195\n","Iteration : 210 ; Train RMSE = 0.7364 ; Test RMSE = 0.9191\n","Iteration : 220 ; Train RMSE = 0.7146 ; Test RMSE = 0.9193\n","Iteration : 230 ; Train RMSE = 0.6928 ; Test RMSE = 0.9201\n","Iteration : 240 ; Train RMSE = 0.6712 ; Test RMSE = 0.9215\n","Iteration : 250 ; Train RMSE = 0.6502 ; Test RMSE = 0.9235\n","Iteration : 260 ; Train RMSE = 0.6298 ; Test RMSE = 0.9258\n","Iteration : 270 ; Train RMSE = 0.6103 ; Test RMSE = 0.9285\n","Iteration : 280 ; Train RMSE = 0.5916 ; Test RMSE = 0.9314\n","Iteration : 290 ; Train RMSE = 0.5739 ; Test RMSE = 0.9346\n","Iteration : 300 ; Train RMSE = 0.5571 ; Test RMSE = 0.9378\n","K : 70\n","Iteration : 10 ; Train RMSE = 0.9670 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9623\n","Iteration : 30 ; Train RMSE = 0.9308 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9244 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9201 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9169 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9142 ; Test RMSE = 0.9470\n","Iteration : 80 ; Train RMSE = 0.9117 ; Test RMSE = 0.9462\n","Iteration : 90 ; Train RMSE = 0.9088 ; Test RMSE = 0.9454\n","Iteration : 100 ; Train RMSE = 0.9054 ; Test RMSE = 0.9444\n","Iteration : 110 ; Train RMSE = 0.9007 ; Test RMSE = 0.9430\n","Iteration : 120 ; Train RMSE = 0.8942 ; Test RMSE = 0.9411\n","Iteration : 130 ; Train RMSE = 0.8854 ; Test RMSE = 0.9383\n","Iteration : 140 ; Train RMSE = 0.8741 ; Test RMSE = 0.9348\n","Iteration : 150 ; Train RMSE = 0.8605 ; Test RMSE = 0.9310\n","Iteration : 160 ; Train RMSE = 0.8451 ; Test RMSE = 0.9273\n","Iteration : 170 ; Train RMSE = 0.8283 ; Test RMSE = 0.9239\n","Iteration : 180 ; Train RMSE = 0.8101 ; Test RMSE = 0.9210\n","Iteration : 190 ; Train RMSE = 0.7906 ; Test RMSE = 0.9187\n","Iteration : 200 ; Train RMSE = 0.7700 ; Test RMSE = 0.9168\n","Iteration : 210 ; Train RMSE = 0.7485 ; Test RMSE = 0.9156\n","Iteration : 220 ; Train RMSE = 0.7263 ; Test RMSE = 0.9149\n","Iteration : 230 ; Train RMSE = 0.7036 ; Test RMSE = 0.9149\n","Iteration : 240 ; Train RMSE = 0.6809 ; Test RMSE = 0.9154\n","Iteration : 250 ; Train RMSE = 0.6582 ; Test RMSE = 0.9164\n","Iteration : 260 ; Train RMSE = 0.6358 ; Test RMSE = 0.9179\n","Iteration : 270 ; Train RMSE = 0.6140 ; Test RMSE = 0.9197\n","Iteration : 280 ; Train RMSE = 0.5928 ; Test RMSE = 0.9219\n","Iteration : 290 ; Train RMSE = 0.5723 ; Test RMSE = 0.9242\n","Iteration : 300 ; Train RMSE = 0.5527 ; Test RMSE = 0.9267\n","K : 90\n","Iteration : 10 ; Train RMSE = 0.9670 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9310 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9247 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9205 ; Test RMSE = 0.9493\n","Iteration : 60 ; Train RMSE = 0.9174 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9149 ; Test RMSE = 0.9470\n","Iteration : 80 ; Train RMSE = 0.9125 ; Test RMSE = 0.9462\n","Iteration : 90 ; Train RMSE = 0.9101 ; Test RMSE = 0.9454\n","Iteration : 100 ; Train RMSE = 0.9070 ; Test RMSE = 0.9445\n","Iteration : 110 ; Train RMSE = 0.9030 ; Test RMSE = 0.9433\n","Iteration : 120 ; Train RMSE = 0.8975 ; Test RMSE = 0.9414\n","Iteration : 130 ; Train RMSE = 0.8899 ; Test RMSE = 0.9388\n","Iteration : 140 ; Train RMSE = 0.8799 ; Test RMSE = 0.9355\n","Iteration : 150 ; Train RMSE = 0.8677 ; Test RMSE = 0.9318\n","Iteration : 160 ; Train RMSE = 0.8538 ; Test RMSE = 0.9282\n","Iteration : 170 ; Train RMSE = 0.8384 ; Test RMSE = 0.9248\n","Iteration : 180 ; Train RMSE = 0.8216 ; Test RMSE = 0.9219\n","Iteration : 190 ; Train RMSE = 0.8033 ; Test RMSE = 0.9193\n","Iteration : 200 ; Train RMSE = 0.7836 ; Test RMSE = 0.9172\n","Iteration : 210 ; Train RMSE = 0.7626 ; Test RMSE = 0.9155\n","Iteration : 220 ; Train RMSE = 0.7406 ; Test RMSE = 0.9144\n","Iteration : 230 ; Train RMSE = 0.7178 ; Test RMSE = 0.9139\n","Iteration : 240 ; Train RMSE = 0.6944 ; Test RMSE = 0.9139\n","Iteration : 250 ; Train RMSE = 0.6709 ; Test RMSE = 0.9145\n","Iteration : 260 ; Train RMSE = 0.6474 ; Test RMSE = 0.9155\n","Iteration : 270 ; Train RMSE = 0.6241 ; Test RMSE = 0.9169\n","Iteration : 280 ; Train RMSE = 0.6012 ; Test RMSE = 0.9186\n","Iteration : 290 ; Train RMSE = 0.5790 ; Test RMSE = 0.9206\n","Iteration : 300 ; Train RMSE = 0.5575 ; Test RMSE = 0.9228\n","K : 110\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9421 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9311 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9248 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9207 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9177 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9153 ; Test RMSE = 0.9470\n","Iteration : 80 ; Train RMSE = 0.9132 ; Test RMSE = 0.9463\n","Iteration : 90 ; Train RMSE = 0.9109 ; Test RMSE = 0.9457\n","Iteration : 100 ; Train RMSE = 0.9083 ; Test RMSE = 0.9449\n","Iteration : 110 ; Train RMSE = 0.9049 ; Test RMSE = 0.9439\n","Iteration : 120 ; Train RMSE = 0.9002 ; Test RMSE = 0.9424\n","Iteration : 130 ; Train RMSE = 0.8936 ; Test RMSE = 0.9402\n","Iteration : 140 ; Train RMSE = 0.8847 ; Test RMSE = 0.9372\n","Iteration : 150 ; Train RMSE = 0.8735 ; Test RMSE = 0.9336\n","Iteration : 160 ; Train RMSE = 0.8604 ; Test RMSE = 0.9298\n","Iteration : 170 ; Train RMSE = 0.8458 ; Test RMSE = 0.9262\n","Iteration : 180 ; Train RMSE = 0.8298 ; Test RMSE = 0.9229\n","Iteration : 190 ; Train RMSE = 0.8124 ; Test RMSE = 0.9200\n","Iteration : 200 ; Train RMSE = 0.7936 ; Test RMSE = 0.9176\n","Iteration : 210 ; Train RMSE = 0.7734 ; Test RMSE = 0.9156\n","Iteration : 220 ; Train RMSE = 0.7519 ; Test RMSE = 0.9141\n","Iteration : 230 ; Train RMSE = 0.7294 ; Test RMSE = 0.9131\n","Iteration : 240 ; Train RMSE = 0.7062 ; Test RMSE = 0.9127\n","Iteration : 250 ; Train RMSE = 0.6824 ; Test RMSE = 0.9129\n","Iteration : 260 ; Train RMSE = 0.6584 ; Test RMSE = 0.9135\n","Iteration : 270 ; Train RMSE = 0.6345 ; Test RMSE = 0.9146\n","Iteration : 280 ; Train RMSE = 0.6109 ; Test RMSE = 0.9160\n","Iteration : 290 ; Train RMSE = 0.5877 ; Test RMSE = 0.9178\n","Iteration : 300 ; Train RMSE = 0.5652 ; Test RMSE = 0.9197\n","K : 130\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9249 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9208 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9179 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9156 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9135 ; Test RMSE = 0.9463\n","Iteration : 90 ; Train RMSE = 0.9114 ; Test RMSE = 0.9457\n","Iteration : 100 ; Train RMSE = 0.9090 ; Test RMSE = 0.9449\n","Iteration : 110 ; Train RMSE = 0.9058 ; Test RMSE = 0.9439\n","Iteration : 120 ; Train RMSE = 0.9014 ; Test RMSE = 0.9425\n","Iteration : 130 ; Train RMSE = 0.8953 ; Test RMSE = 0.9404\n","Iteration : 140 ; Train RMSE = 0.8871 ; Test RMSE = 0.9375\n","Iteration : 150 ; Train RMSE = 0.8768 ; Test RMSE = 0.9341\n","Iteration : 160 ; Train RMSE = 0.8648 ; Test RMSE = 0.9305\n","Iteration : 170 ; Train RMSE = 0.8514 ; Test RMSE = 0.9271\n","Iteration : 180 ; Train RMSE = 0.8366 ; Test RMSE = 0.9238\n","Iteration : 190 ; Train RMSE = 0.8203 ; Test RMSE = 0.9208\n","Iteration : 200 ; Train RMSE = 0.8025 ; Test RMSE = 0.9181\n","Iteration : 210 ; Train RMSE = 0.7830 ; Test RMSE = 0.9158\n","Iteration : 220 ; Train RMSE = 0.7622 ; Test RMSE = 0.9139\n","Iteration : 230 ; Train RMSE = 0.7402 ; Test RMSE = 0.9126\n","Iteration : 240 ; Train RMSE = 0.7172 ; Test RMSE = 0.9117\n","Iteration : 250 ; Train RMSE = 0.6935 ; Test RMSE = 0.9114\n","Iteration : 260 ; Train RMSE = 0.6694 ; Test RMSE = 0.9115\n","Iteration : 270 ; Train RMSE = 0.6452 ; Test RMSE = 0.9122\n","Iteration : 280 ; Train RMSE = 0.6212 ; Test RMSE = 0.9132\n","Iteration : 290 ; Train RMSE = 0.5976 ; Test RMSE = 0.9146\n","Iteration : 300 ; Train RMSE = 0.5746 ; Test RMSE = 0.9163\n","K : 150\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9250 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9209 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9181 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9158 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9138 ; Test RMSE = 0.9464\n","Iteration : 90 ; Train RMSE = 0.9118 ; Test RMSE = 0.9458\n","Iteration : 100 ; Train RMSE = 0.9096 ; Test RMSE = 0.9451\n","Iteration : 110 ; Train RMSE = 0.9068 ; Test RMSE = 0.9441\n","Iteration : 120 ; Train RMSE = 0.9028 ; Test RMSE = 0.9428\n","Iteration : 130 ; Train RMSE = 0.8972 ; Test RMSE = 0.9409\n","Iteration : 140 ; Train RMSE = 0.8896 ; Test RMSE = 0.9381\n","Iteration : 150 ; Train RMSE = 0.8798 ; Test RMSE = 0.9347\n","Iteration : 160 ; Train RMSE = 0.8682 ; Test RMSE = 0.9311\n","Iteration : 170 ; Train RMSE = 0.8552 ; Test RMSE = 0.9276\n","Iteration : 180 ; Train RMSE = 0.8409 ; Test RMSE = 0.9244\n","Iteration : 190 ; Train RMSE = 0.8253 ; Test RMSE = 0.9215\n","Iteration : 200 ; Train RMSE = 0.8082 ; Test RMSE = 0.9190\n","Iteration : 210 ; Train RMSE = 0.7895 ; Test RMSE = 0.9167\n","Iteration : 220 ; Train RMSE = 0.7693 ; Test RMSE = 0.9149\n","Iteration : 230 ; Train RMSE = 0.7478 ; Test RMSE = 0.9135\n","Iteration : 240 ; Train RMSE = 0.7252 ; Test RMSE = 0.9125\n","Iteration : 250 ; Train RMSE = 0.7016 ; Test RMSE = 0.9120\n","Iteration : 260 ; Train RMSE = 0.6775 ; Test RMSE = 0.9120\n","Iteration : 270 ; Train RMSE = 0.6531 ; Test RMSE = 0.9124\n","Iteration : 280 ; Train RMSE = 0.6286 ; Test RMSE = 0.9132\n","Iteration : 290 ; Train RMSE = 0.6043 ; Test RMSE = 0.9143\n","Iteration : 300 ; Train RMSE = 0.5805 ; Test RMSE = 0.9157\n","K : 170\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9210 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9182 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9159 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9140 ; Test RMSE = 0.9464\n","Iteration : 90 ; Train RMSE = 0.9121 ; Test RMSE = 0.9458\n","Iteration : 100 ; Train RMSE = 0.9100 ; Test RMSE = 0.9451\n","Iteration : 110 ; Train RMSE = 0.9073 ; Test RMSE = 0.9443\n","Iteration : 120 ; Train RMSE = 0.9037 ; Test RMSE = 0.9430\n","Iteration : 130 ; Train RMSE = 0.8984 ; Test RMSE = 0.9412\n","Iteration : 140 ; Train RMSE = 0.8913 ; Test RMSE = 0.9386\n","Iteration : 150 ; Train RMSE = 0.8820 ; Test RMSE = 0.9353\n","Iteration : 160 ; Train RMSE = 0.8708 ; Test RMSE = 0.9317\n","Iteration : 170 ; Train RMSE = 0.8583 ; Test RMSE = 0.9282\n","Iteration : 180 ; Train RMSE = 0.8444 ; Test RMSE = 0.9249\n","Iteration : 190 ; Train RMSE = 0.8290 ; Test RMSE = 0.9219\n","Iteration : 200 ; Train RMSE = 0.8122 ; Test RMSE = 0.9191\n","Iteration : 210 ; Train RMSE = 0.7937 ; Test RMSE = 0.9167\n","Iteration : 220 ; Train RMSE = 0.7738 ; Test RMSE = 0.9146\n","Iteration : 230 ; Train RMSE = 0.7525 ; Test RMSE = 0.9130\n","Iteration : 240 ; Train RMSE = 0.7302 ; Test RMSE = 0.9118\n","Iteration : 250 ; Train RMSE = 0.7069 ; Test RMSE = 0.9111\n","Iteration : 260 ; Train RMSE = 0.6830 ; Test RMSE = 0.9110\n","Iteration : 270 ; Train RMSE = 0.6588 ; Test RMSE = 0.9112\n","Iteration : 280 ; Train RMSE = 0.6346 ; Test RMSE = 0.9119\n","Iteration : 290 ; Train RMSE = 0.6105 ; Test RMSE = 0.9129\n","Iteration : 300 ; Train RMSE = 0.5868 ; Test RMSE = 0.9142\n","K : 190\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9161 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9143 ; Test RMSE = 0.9464\n","Iteration : 90 ; Train RMSE = 0.9125 ; Test RMSE = 0.9459\n","Iteration : 100 ; Train RMSE = 0.9106 ; Test RMSE = 0.9453\n","Iteration : 110 ; Train RMSE = 0.9083 ; Test RMSE = 0.9446\n","Iteration : 120 ; Train RMSE = 0.9051 ; Test RMSE = 0.9435\n","Iteration : 130 ; Train RMSE = 0.9006 ; Test RMSE = 0.9419\n","Iteration : 140 ; Train RMSE = 0.8943 ; Test RMSE = 0.9396\n","Iteration : 150 ; Train RMSE = 0.8859 ; Test RMSE = 0.9366\n","Iteration : 160 ; Train RMSE = 0.8756 ; Test RMSE = 0.9331\n","Iteration : 170 ; Train RMSE = 0.8637 ; Test RMSE = 0.9295\n","Iteration : 180 ; Train RMSE = 0.8505 ; Test RMSE = 0.9262\n","Iteration : 190 ; Train RMSE = 0.8358 ; Test RMSE = 0.9230\n","Iteration : 200 ; Train RMSE = 0.8195 ; Test RMSE = 0.9200\n","Iteration : 210 ; Train RMSE = 0.8016 ; Test RMSE = 0.9173\n","Iteration : 220 ; Train RMSE = 0.7821 ; Test RMSE = 0.9150\n","Iteration : 230 ; Train RMSE = 0.7611 ; Test RMSE = 0.9130\n","Iteration : 240 ; Train RMSE = 0.7389 ; Test RMSE = 0.9115\n","Iteration : 250 ; Train RMSE = 0.7157 ; Test RMSE = 0.9105\n","Iteration : 260 ; Train RMSE = 0.6917 ; Test RMSE = 0.9100\n","Iteration : 270 ; Train RMSE = 0.6673 ; Test RMSE = 0.9100\n","Iteration : 280 ; Train RMSE = 0.6427 ; Test RMSE = 0.9104\n","Iteration : 290 ; Train RMSE = 0.6183 ; Test RMSE = 0.9112\n","Iteration : 300 ; Train RMSE = 0.5941 ; Test RMSE = 0.9122\n","K : 210\n","Iteration : 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9480\n","Iteration : 70 ; Train RMSE = 0.9162 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9144 ; Test RMSE = 0.9465\n","Iteration : 90 ; Train RMSE = 0.9126 ; Test RMSE = 0.9459\n","Iteration : 100 ; Train RMSE = 0.9108 ; Test RMSE = 0.9453\n","Iteration : 110 ; Train RMSE = 0.9084 ; Test RMSE = 0.9445\n","Iteration : 120 ; Train RMSE = 0.9053 ; Test RMSE = 0.9434\n","Iteration : 130 ; Train RMSE = 0.9008 ; Test RMSE = 0.9418\n","Iteration : 140 ; Train RMSE = 0.8945 ; Test RMSE = 0.9394\n","Iteration : 150 ; Train RMSE = 0.8861 ; Test RMSE = 0.9364\n","Iteration : 160 ; Train RMSE = 0.8760 ; Test RMSE = 0.9328\n","Iteration : 170 ; Train RMSE = 0.8644 ; Test RMSE = 0.9293\n","Iteration : 180 ; Train RMSE = 0.8516 ; Test RMSE = 0.9260\n","Iteration : 190 ; Train RMSE = 0.8376 ; Test RMSE = 0.9230\n","Iteration : 200 ; Train RMSE = 0.8222 ; Test RMSE = 0.9202\n","Iteration : 210 ; Train RMSE = 0.8052 ; Test RMSE = 0.9176\n","Iteration : 220 ; Train RMSE = 0.7865 ; Test RMSE = 0.9153\n","Iteration : 230 ; Train RMSE = 0.7663 ; Test RMSE = 0.9134\n","Iteration : 240 ; Train RMSE = 0.7446 ; Test RMSE = 0.9118\n","Iteration : 250 ; Train RMSE = 0.7218 ; Test RMSE = 0.9108\n","Iteration : 260 ; Train RMSE = 0.6981 ; Test RMSE = 0.9102\n","Iteration : 270 ; Train RMSE = 0.6738 ; Test RMSE = 0.9101\n","Iteration : 280 ; Train RMSE = 0.6492 ; Test RMSE = 0.9104\n","Iteration : 290 ; Train RMSE = 0.6247 ; Test RMSE = 0.9111\n","Iteration : 300 ; Train RMSE = 0.6004 ; Test RMSE = 0.9122\n","K : 230\n","Iteration : 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9551\n","Iteration : 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9481\n","Iteration : 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9471\n","Iteration : 80 ; Train RMSE = 0.9146 ; Test RMSE = 0.9465\n","Iteration : 90 ; Train RMSE = 0.9130 ; Test RMSE = 0.9460\n","Iteration : 100 ; Train RMSE = 0.9113 ; Test RMSE = 0.9455\n","Iteration : 110 ; Train RMSE = 0.9092 ; Test RMSE = 0.9449\n","Iteration : 120 ; Train RMSE = 0.9066 ; Test RMSE = 0.9439\n","Iteration : 130 ; Train RMSE = 0.9027 ; Test RMSE = 0.9426\n","Iteration : 140 ; Train RMSE = 0.8973 ; Test RMSE = 0.9406\n","Iteration : 150 ; Train RMSE = 0.8898 ; Test RMSE = 0.9378\n","Iteration : 160 ; Train RMSE = 0.8802 ; Test RMSE = 0.9344\n","Iteration : 170 ; Train RMSE = 0.8688 ; Test RMSE = 0.9307\n","Iteration : 180 ; Train RMSE = 0.8562 ; Test RMSE = 0.9272\n","Iteration : 190 ; Train RMSE = 0.8424 ; Test RMSE = 0.9240\n","Iteration : 200 ; Train RMSE = 0.8273 ; Test RMSE = 0.9211\n","Iteration : 210 ; Train RMSE = 0.8106 ; Test RMSE = 0.9184\n","Iteration : 220 ; Train RMSE = 0.7925 ; Test RMSE = 0.9161\n","Iteration : 230 ; Train RMSE = 0.7728 ; Test RMSE = 0.9142\n","Iteration : 240 ; Train RMSE = 0.7517 ; Test RMSE = 0.9126\n","Iteration : 250 ; Train RMSE = 0.7294 ; Test RMSE = 0.9115\n","Iteration : 260 ; Train RMSE = 0.7061 ; Test RMSE = 0.9108\n","Iteration : 270 ; Train RMSE = 0.6821 ; Test RMSE = 0.9107\n","Iteration : 280 ; Train RMSE = 0.6576 ; Test RMSE = 0.9109\n","Iteration : 290 ; Train RMSE = 0.6330 ; Test RMSE = 0.9116\n","Iteration : 300 ; Train RMSE = 0.6085 ; Test RMSE = 0.9126\n","K : 250\n","Iteration : 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n","Iteration : 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n","Iteration : 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n","Iteration : 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n","Iteration : 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n","Iteration : 60 ; Train RMSE = 0.9185 ; Test RMSE = 0.9481\n","Iteration : 70 ; Train RMSE = 0.9164 ; Test RMSE = 0.9472\n","Iteration : 80 ; Train RMSE = 0.9147 ; Test RMSE = 0.9465\n","Iteration : 90 ; Train RMSE = 0.9131 ; Test RMSE = 0.9460\n","Iteration : 100 ; Train RMSE = 0.9114 ; Test RMSE = 0.9455\n","Iteration : 110 ; Train RMSE = 0.9095 ; Test RMSE = 0.9449\n","Iteration : 120 ; Train RMSE = 0.9069 ; Test RMSE = 0.9440\n","Iteration : 130 ; Train RMSE = 0.9033 ; Test RMSE = 0.9427\n","Iteration : 140 ; Train RMSE = 0.8980 ; Test RMSE = 0.9408\n","Iteration : 150 ; Train RMSE = 0.8908 ; Test RMSE = 0.9380\n","Iteration : 160 ; Train RMSE = 0.8814 ; Test RMSE = 0.9347\n","Iteration : 170 ; Train RMSE = 0.8704 ; Test RMSE = 0.9310\n","Iteration : 180 ; Train RMSE = 0.8580 ; Test RMSE = 0.9275\n","Iteration : 190 ; Train RMSE = 0.8444 ; Test RMSE = 0.9243\n","Iteration : 200 ; Train RMSE = 0.8295 ; Test RMSE = 0.9214\n","Iteration : 210 ; Train RMSE = 0.8132 ; Test RMSE = 0.9187\n","Iteration : 220 ; Train RMSE = 0.7953 ; Test RMSE = 0.9164\n","Iteration : 230 ; Train RMSE = 0.7758 ; Test RMSE = 0.9143\n","Iteration : 240 ; Train RMSE = 0.7549 ; Test RMSE = 0.9127\n","Iteration : 250 ; Train RMSE = 0.7327 ; Test RMSE = 0.9115\n","Iteration : 260 ; Train RMSE = 0.7095 ; Test RMSE = 0.9108\n","Iteration : 270 ; Train RMSE = 0.6856 ; Test RMSE = 0.9105\n","Iteration : 280 ; Train RMSE = 0.6612 ; Test RMSE = 0.9107\n","Iteration : 290 ; Train RMSE = 0.6366 ; Test RMSE = 0.9113\n","Iteration : 300 ; Train RMSE = 0.6121 ; Test RMSE = 0.9122\n"]}]},{"cell_type":"code","source":["summary = []\n","for i in range (len(result)) :\n","  RMSE = []\n","  for result in results[i] :\n","    # result에서 0번째는 그냥 인덱스, 1번째는 train에 대한 RMSE, 2번째는 test에 대한 RMSE\n","    RMSE.append(result[2])\n","  min = np.min(RMSE)\n","  # 몇 번째의 iteration이 최소의 RMSE를 갖게 되는지\n","  j = RMSE.index(min)\n","  # summary에 K값, 최적의 RMSE iteration 값, 최소의 RMSE 값\n","  summary.append([index[i], j+1, RMSE[j]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"6wrCbz_HoFgx","executionInfo":{"status":"error","timestamp":1698897117441,"user_tz":-540,"elapsed":6,"user":{"displayName":"이윤서","userId":"07220253656566703671"}},"outputId":"72adfa61-b669-4204-cdc7-566b2ca40a09"},"execution_count":6,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-71a2c8948b62>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# result에서 0번째는 그냥 인덱스, 1번째는 train에 대한 RMSE, 2번째는 test에 대한 RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mRMSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["# 그래프를 보면 K가 150 근처에서 최적값"],"metadata":{"id":"0x51D6eatFHn","executionInfo":{"status":"aborted","timestamp":1698897117441,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤서","userId":"07220253656566703671"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary"],"metadata":{"id":"RM9Q3fDTpZWr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.6. MF와 SVD\n","### 4.6.1. MF\n","* 원래의 행렬을 두 개의 행렬로 나눈다\n","* null 값을 0으로 표현하긴 하지만,  \n","  P와 Q에 대해 연산할 때 0에 대한 값은 빼고 계산하기 때문에   \n","  사실 null 값은 제외하고 계산하는 구조\n","* 원래 행렬에 null 값이 있다하더라도, P와 Q 행렬은 null 없이 계산하기 때문에  \n","  학습이 끝나고 나면 원래 행렬의 null 값에 대해서도 상당히 정확하게 예측할 수 있음\n","\n","### 4.6.2. SVD\n","* 원래의 행렬을 세 개의 행렬로 나누어 학습한 뒤 다시 원래의 행렬로 재현\n","* 원래 행렬의 null값을 허용하지 않기 때문에   \n","  원래 행렬에 없던 값을 예측하는 데에는 문제가 있다\n","* null값을 0으로 대체하면   \n","  0 또한 하나의 값으로 인식하고 이 0값에 대해 예측하기 위해 노력하기 때문에  \n","  0이 아니라 0에 가까운 값으로 예측"],"metadata":{"id":"hA6NioWsuzNq"}},{"cell_type":"code","source":[],"metadata":{"id":"PrlP3yqVuxTc"},"execution_count":null,"outputs":[]}]}